from utils import *
from sklearn.cluster import MiniBatchKMeans


class PQ_IVF:
    def __init__(self,file_path,chunk_size,K_means_n_clusters,K_means_max_iter):
        '''
        file_path:Path to .bin file to of raw data to apply PQ as generated by insert binary function in API class
        chunk_size: no of records to be processing together in while performing kmeans
        K_means_n_clusters: No of Kmeans Clusters
        K_means_max_iter: max iteration by kmeans default [100] in sklearn

        '''
        self.file_path=file_path  # Original Data
        self.chunk_size=chunk_size
        self.K_means_n_clusters=K_means_n_clusters
        self.K_means_max_iter=K_means_max_iter
        

    def PQ_IVF_index(self):
        print("---PQ_IVF_index()----")
        # Clustering Data from file
        # We need to Read Data from File chunk by chunk
        file_size = os.path.getsize(self.file_path)
        record_size=struct.calcsize(f"I{70}f")
        n_records=file_size/record_size
        no_chunks=math.ceil(n_records/self.chunk_size)


        # Initialize MiniBatchKMeans to clustering
        kmeans = MiniBatchKMeans(n_clusters=self.K_means_n_clusters, batch_size=self.chunk_size, max_iter=self.K_means_max_iter,n_init=4,random_state=42)
        
        print("***********Reading File in build index()*****************")
        print("data.bin file size (R=284 Byte): ",file_size)
        print("n_records",n_records)
        print("chuck size",self.chunk_size)
        print("No of Chunks",no_chunks)
        for i in range(no_chunks):
            print("Reading Chunk",i,"....")
            data_chunk=read_binary_file_chunk(file_path=self.file_path,record_format=f"I{70}f",start_index=i*self.chunk_size,chunk_size=self.chunk_size) #[{"id":,"embed":[]}]
            # Extract Data
            chunk_vectors=np.array([entry['embed'] for entry in data_chunk])
            print(chunk_vectors.shape)
            kmeans.partial_fit(chunk_vectors)
            centroids = kmeans.cluster_centers_
            print(centroids.shape)
            # if(data_chunk is None):
            #     # If out of index but not needed here
            #     break
            # data_chunk=extract_embeds_array(data_chunk)
            # for data

            # kmeans.partial_fit(batch)

        # labels = kmeans.predict(X)
        return 




    
  

def semantic_query_pq_ivf(query):
    pass