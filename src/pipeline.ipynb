{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Modules.LSH import semantic_query_lsh\n",
    "# from Modules.LSH import LSH\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"./random_data.txt\"\n",
    "# read_data = np.loadtxt(file_path)\n",
    "# plane_norms = LSH(read_data, 8)\n",
    "# query=[read_data[0]]\n",
    "# folder_name = \"bucket_files\"\n",
    "# result = semantic_query_lsh(query, plane_norms,folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from Modules.LSH import*\n",
    "\n",
    "\n",
    "datafile_path=\"../DataBase/random_data_1000.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 70)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test LSH_index\n",
    "# Read Data From File\n",
    "data_path = datafile_path\n",
    "read_data = np.loadtxt(data_path)\n",
    "print(read_data.shape)\n",
    "\n",
    "\n",
    "# Layer(1)\n",
    "# Convert to Dictionary TODO Take From Zeyad\n",
    "level_1_in=array_to_dictionary(read_data)\n",
    "\n",
    "Level_1_path = \"../DataBase/Level1\"\n",
    "\n",
    "check_dir(Level_1_path)\n",
    "\n",
    "level_1_planes=LSH_index(data=level_1_in, nbits=8,index_path=Level_1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer(2)\n",
    "# OnEach Bucket Apply LSH\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(Level_1_path)\n",
    "\n",
    "Level_2_path = \"../DataBase/Level2\"\n",
    "\n",
    "check_dir(Level_2_path)\n",
    "\n",
    "level_2_planes={}\n",
    "# Loop over the files\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(Level_1_path, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        # TODO Zeyad\n",
    "        # Read Data\n",
    "        read_data_2 = np.array(np.loadtxt(file_path,dtype=int)).reshape((-1,1))\n",
    "        # no_of_elements=1 if not read_data.shape else read_data.shape[0]\n",
    "        # # break\n",
    "        vectors=np.array([[]])\n",
    "    \n",
    "\n",
    "        for id in read_data_2:\n",
    "            vector=get_vector_from_id(datafile_path,id)\n",
    "            # If vectors is still an empty array, initialize it with the first vector\n",
    "            if vectors.size == 0:\n",
    "                vectors = vector.reshape(1, -1)\n",
    "            else:\n",
    "                vectors = np.insert(vectors, len(vectors), [vector], axis=0)\n",
    "        \n",
    "        level_2_in=array_to_dictionary(values=vectors,keys=np.hstack(read_data_2))\n",
    "\n",
    "        # # Apply LSH on this Bucket\n",
    "        level_2_planes[file_name[:-4]]=LSH_index(data=level_2_in, nbits=8,index_path='../DataBase/Level2/'+file_name[:-4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8  47 177 504 662 763 858 888 974]\n",
      "00111011\n",
      "177\n",
      "11110111\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "query=read_data[177]\n",
    "# print(query)\n",
    "# Layer (1)\n",
    "bucket = semantic_query_lsh(query=query,plane_norms=level_1_planes,index_path=\"../DataBase/Level1\")\n",
    "print(bucket)\n",
    "\n",
    "# Layer(2)\n",
    "bucket = semantic_query_lsh(query=query,plane_norms=level_2_planes[bucket],index_path=\"../DataBase/Level2/\"+bucket)\n",
    "print(bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[304 365 813 824]\n",
      "11101001\n",
      "304\n",
      "01101000\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "query=read_data[304]\n",
    "# print(query)\n",
    "# Layer (1)\n",
    "bucket = semantic_query_lsh(query=query,plane_norms=level_1_planes,index_path=\"../DataBase/Level1\")\n",
    "print(bucket)\n",
    "\n",
    "# Layer(2)\n",
    "bucket = semantic_query_lsh(query=query,plane_norms=level_2_planes[bucket],index_path=\"../DataBase/Level2/\"+bucket)\n",
    "print(bucket)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
