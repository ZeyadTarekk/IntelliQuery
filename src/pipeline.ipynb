{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# from Modules.LSH import semantic_query_lsh\n",
                "# from Modules.LSH import LSH\n",
                "\n",
                "\n",
                "# import numpy as np\n",
                "# import os"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# file_path = \"./random_data.txt\"\n",
                "# read_data = np.loadtxt(file_path)\n",
                "# plane_norms = LSH(read_data, 8)\n",
                "# query=[read_data[0]]\n",
                "# folder_name = \"bucket_files\"\n",
                "# result = semantic_query_lsh(query, plane_norms,folder_name)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The autoreload extension is already loaded. To reload it, use:\n",
                        "  %reload_ext autoreload\n"
                    ]
                }
            ],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "\n",
                "from utils import *\n",
                "from Modules.LSH import*\n",
                "from api import *\n",
                "from evaluation import *\n",
                "from worst_case_implementation import VecDBWorst\n",
                "\n",
                "\n",
                "# datafile_path=\"../DataBase/random_data_10000.txt\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Inserted 10000 records. Total records inserted: 10000\n",
                        "Insertion complete.\n"
                    ]
                }
            ],
            "source": [
                "data_file='./DataBase/data.bin'\n",
                "Level_1_path='./DataBase/Level1'\n",
                "Level_2_path='./DataBase/Level2'\n",
                "Level_3_path='./DataBase/Level3'\n",
                "\n",
                "Level_1_nbits=8\n",
                "Level_2_nbits=3\n",
                "Level_3_nbits=3\n",
                "\n",
                "data_api = DataApi(data_file)\n",
                "data_api.generate_data_file(10000)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Test LSH_index\n",
                "# Read Data From File\n",
                "read_data = data_api.get_top_k_records(10000)\n",
                "\n",
                "\n",
                "# Layer(1)\n",
                "level_1_in=read_data\n",
                "# TODO: Save Planes to be used in query Search\n",
                "level_1_planes=LSH_index(data=level_1_in, nbits=Level_1_nbits,index_path=Level_1_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Layer(2)\n",
                "# On Each Bucket Apply LSH\n",
                "\n",
                "# List all files in the directory\n",
                "files = os.listdir(Level_1_path)\n",
                "\n",
                "# TODO: Save Planes to be used in query Search\n",
                "level_2_planes={}\n",
                "\n",
                "# Loop over the files\n",
                "for file_name in files:\n",
                "    file_path = os.path.join(Level_1_path, file_name)\n",
                "    \n",
                "    if os.path.isfile(file_path):\n",
                "        # Read Data\n",
                "        read_data_2 = np.loadtxt(file_path,dtype=int,ndmin=1)\n",
                "\n",
                "        level_2_in=data_api.get_multiple_records_by_ids(read_data_2-1)\n",
                "        # level_2_in = array_to_dictionary(values=vectors,keys=np.hstack(read_data_2))\n",
                "\n",
                "        # # Apply LSH on this Bucket\n",
                "        # level_2=arr[level_1]\n",
                "        level_2_planes[file_name[:-4]]=LSH_index(data=level_2_in.values(), nbits=Level_2_nbits,index_path=Level_2_path+'/' + file_name[:-4])\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Layer(3)\n",
                "# On Each Bucket Apply LSH\n",
                "\n",
                "# List all files in the directory\n",
                "folders = os.listdir(Level_2_path)\n",
                "\n",
                "# TODO: Save Planes to be used in query Search\n",
                "level_3_planes={}\n",
                "# file_3=folder{}\n",
                "# Loop over the folders\n",
                "for folder_name in folders:\n",
                "    level_3_planes[folder_name]={}\n",
                "    folder_path = os.path.join(Level_2_path, folder_name)\n",
                "    files = os.listdir(folder_path)\n",
                "    # Loop over the files\n",
                "    for file_name in files:\n",
                "        file_path = os.path.join(folder_path, file_name)\n",
                "        \n",
                "        if os.path.isfile(file_path):\n",
                "            # Read Data\n",
                "            read_data_3 = np.loadtxt(file_path,dtype=int,ndmin=1)\n",
                "\n",
                "            level_3_in=data_api.get_multiple_records_by_ids(read_data_3)\n",
                "\n",
                "            # # Apply LSH on this Bucket\n",
                "            level_3_planes[folder_name][file_name[:-4]]=LSH_index(data=level_3_in.values(), nbits=Level_3_nbits,index_path=Level_3_path+'/'+folder_name+'/' + file_name[:-4])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "bucket of level 1:  11110011\n",
                        "=====================================\n",
                        "bucket of level 2:  001\n",
                        "=====================================\n",
                        "bucket of level 3:  100\n",
                        "Length of level 3 189\n",
                        "Indices of level 3 [  36   51  104  159  266  357  372  385  434  465  510  671  702  707\n",
                        "  720  822  824  834  863  938 1034 1044 1165 1248 1264 1438 1505 1565\n",
                        " 1613 1683 1712 1719 1771 1798 1812 1843 1953 2191 2238 2266 2330 2353\n",
                        " 2594 2602 2624 2669 2730 2744 2825 2880 2894 2915 2942 2944 3080 3168\n",
                        " 3286 3351 3490 3645 3648 3735 3798 3851 3859 3911 3986 4026 4030 4065\n",
                        " 4121 4134 4187 4211 4232 4260 4391 4399 4476 4477 4489 4492 4545 4554\n",
                        " 4591 4605 4660 4792 4905 4937 4953 4954 4970 4986 4987 5228 5249 5329\n",
                        " 5398 5454 5471 5495 5584 5708 5712 5725 5744 5799 5899 5900 5908 5952\n",
                        " 5987 6049 6072 6096 6144 6184 6209 6287 6344 6399 6479 6495 6536 6544\n",
                        " 6662 6693 6848 6880 6915 6962 7080 7085 7187 7199 7213 7240 7390 7404\n",
                        " 7417 7442 7531 7538 7554 7584 7625 7664 7708 7721 7765 7768 7808 7827\n",
                        " 7955 8101 8170 8279 8284 8380 8444 8446 8454 8481 8552 8560 8565 8586\n",
                        " 8676 8700 8761 8792 8912 8935 9007 9150 9336 9352 9354 9367 9586 9662\n",
                        " 9745 9762 9794 9801 9859 9948 9972]\n",
                        "=====================================\n",
                        "target_vector [0.5522450804710388, 0.8917692303657532, 0.7913368344306946, 0.6000004410743713, 0.2616525888442993, 0.9615220427513123, 0.4808562695980072, 0.6019359827041626, 0.07978673279285431, 0.30365362763404846, 0.7390730381011963, 0.2133997678756714, 0.36366748809814453, 0.1835469752550125, 0.20069865882396698, 0.13891369104385376, 0.11978743225336075, 0.3913387358188629, 0.002954070921987295, 0.5194749236106873, 0.37845972180366516, 0.9680533409118652, 0.6960610747337341, 0.8805666565895081, 0.06497178226709366, 0.5662519335746765, 0.04004804417490959, 0.2919067144393921, 0.737677812576294, 0.10855083167552948, 0.3745698928833008, 0.37776005268096924, 0.9178327322006226, 0.7241680026054382, 0.12325477600097656, 0.3273957073688507, 0.9901415109634399, 0.4085298478603363, 0.6129018068313599, 0.1801413595676422, 0.9952824711799622, 0.3938077688217163, 0.913888692855835, 0.11249328404664993, 0.14214684069156647, 0.6679161787033081, 0.9495717287063599, 0.4362204968929291, 0.3122316896915436, 0.6952698230743408, 0.8448274731636047, 0.965186595916748, 0.35632771253585815, 0.9069381952285767, 0.42551901936531067, 0.9420151710510254, 0.022108066827058792, 0.6098361611366272, 0.897776186466217, 0.4446363151073456, 0.7102886438369751, 0.5624412894248962, 0.5420237183570862, 0.3291500210762024, 0.2226945161819458, 0.6429535150527954, 0.5322402119636536, 0.09856311231851578, 0.5489377379417419, 0.5590397715568542]\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "unhashable type: 'slice'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[1;32md:\\Semantic-Search-Engine\\pipeline.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Semantic-Search-Engine/pipeline.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m index_result_3\u001b[39m=\u001b[39mdata_api\u001b[39m.\u001b[39mget_multiple_records_by_ids(index_result_3)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Semantic-Search-Engine/pipeline.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m level3_res_vectors\u001b[39m=\u001b[39m[entry[\u001b[39m'\u001b[39m\u001b[39membed\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m index_result_3\u001b[39m.\u001b[39mvalues()]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Semantic-Search-Engine/pipeline.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m top_result,_\u001b[39m=\u001b[39mget_top_k_similar(query,index_result_3,\u001b[39m10\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Semantic-Search-Engine/pipeline.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# print(\"Top k results: \",top_result[0])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Semantic-Search-Engine/pipeline.ipynb#X10sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# print(\"=====================================\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Semantic-Search-Engine/pipeline.ipynb#X10sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# # get the intersection of the two lists level 2 and level3\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Semantic-Search-Engine/pipeline.ipynb#X10sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# count = sum(element in index_result_2 for element in index_result_3)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Semantic-Search-Engine/pipeline.ipynb#X10sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# print(\"Intersection of the two layers: \",count)\u001b[39;00m\n",
                        "File \u001b[1;32md:\\Semantic-Search-Engine\\Modules\\LSH.py:145\u001b[0m, in \u001b[0;36mget_top_k_similar\u001b[1;34m(target_vector, data, k)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39m# Calculate cosine similarities using vectorized operations\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtarget_vector\u001b[39m\u001b[39m\"\u001b[39m,target_vector)\n\u001b[1;32m--> 145\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m,data[\u001b[39m1\u001b[39;49m:\u001b[39m5\u001b[39;49m])\n\u001b[0;32m    146\u001b[0m similarities \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39marray([cosine(target_vector, vector) \u001b[39mfor\u001b[39;00m vector \u001b[39min\u001b[39;00m data])\n\u001b[0;32m    148\u001b[0m \u001b[39m# Find the indices of the top k most similar vectors\u001b[39;00m\n",
                        "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'slice'"
                    ]
                }
            ],
            "source": [
                "# Query\n",
                "query=data_api.get_record_by_id(5)[5]['embed']\n",
                "# Layer (1)\n",
                "bucket_1,index_result_1 = semantic_query_lsh(query=query,plane_norms=level_1_planes,index_path=Level_1_path)\n",
                "print(\"bucket of level 1: \",bucket_1)\n",
                "# print(\"Length of level 1\",len(index_result_1))\n",
                "# print(\"Indices of level 1\",index_result_1)\n",
                "print(\"=====================================\")\n",
                "\n",
                "# Layer(2)\n",
                "bucket_2,index_result_2 = semantic_query_lsh(query=query,plane_norms=level_2_planes[bucket_1],index_path=Level_2_path+\"/\"+bucket_1)\n",
                "print(\"bucket of level 2: \",bucket_2)\n",
                "# print(\"Length of level 2\",len(index_result_2))\n",
                "# print(\"Indices of level 2\",index_result_2)\n",
                "print(\"=====================================\")\n",
                "\n",
                "# Layer(3)\n",
                "bucket_3,index_result_3 = semantic_query_lsh(query=query,plane_norms=level_3_planes[bucket_1][bucket_2],index_path=Level_3_path+\"/\"+bucket_1+'/'+bucket_2)\n",
                "print(\"bucket of level 3: \",bucket_3)\n",
                "print(\"Length of level 3\",len(index_result_3))\n",
                "print(\"Indices of level 3\",index_result_3)\n",
                "\n",
                "\n",
                "print(\"=====================================\")\n",
                "\n",
                "# get top 10 results from the last layer\n",
                "index_result_3=data_api.get_multiple_records_by_ids(index_result_3)\n",
                "level3_res_vectors=[entry['embed'] for entry in index_result_3.values()]\n",
                "top_result,_=get_top_k_similar(query,index_result_3,10)\n",
                "# print(\"Top k results: \",top_result[0])\n",
                "# print(\"=====================================\")\n",
                "# # get the intersection of the two lists level 2 and level3\n",
                "# count = sum(element in index_result_2 for element in index_result_3)\n",
                "# print(\"Intersection of the two layers: \",count)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np_rows = np.array([record['embed'] for record in read_data if 'embed' in record])\n",
                "# temp=[5, 966, 536, 1088, 5073, 5549]\n",
                "index_result_3_minus_one = [id - 1 for id in top_results[0]]\n",
                "res=run_queries(index_result_3_minus_one, np_rows, top_k=len(top_results), num_runs=1,query=np.array([query]))\n",
                "print(eval(res))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "db = VecDBWorst()\n",
                "# records_np = np.random.random((10000, 70))\n",
                "records_np = np.array([record['embed'] for record in read_data if 'embed' in record])\n",
                "\n",
                "# records_dict = [{\"id\": i, \"embed\": list(row)} for i, row in enumerate(records_np)]\n",
                "records_dict=read_data\n",
                "\n",
                "# _len = len(records_np)\n",
                "db.insert_records(records_dict)\n",
                "db_ids=db.retrive(query, top_k=1)\n",
                "db_ids_minus_one = [id - 1 for id in db_ids]\n",
                "res = run_queries(db_ids_minus_one, records_np, 1, 1, np.array([query]))\n",
                "print(eval(res))\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
